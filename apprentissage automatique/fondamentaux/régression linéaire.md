# R√©gression lin√©aire (en cours)
```
Les formules math√©matiques s'affichent dans Chrome avec l'extension ¬´ MathJax 3 Plugin for Github 
[https://chrome.google.com/webstore/detail/mathjax-3-plugin-for-gith/peoghobgdhejhcmgoppjpjcidngdfkod]¬ª.
```

## Introduction

M√©mo sur les fondamentaux de la r√©gression lin√©aire.

## Jeu de donn√©es (DATASET)

- m = nombre de lignes (observations),
- n = nombre de param√®tres (caract√©ristiques).

### Vecteurs y et x 

$y \in \mathbb{R}^{(m\times1)}$

$y=\left[\begin{matrix} y^{(1)} \\ y^{(1)} \\ \vdots \\ y^{(m)} \end{matrix} \right]$

$x \in \mathbb{R}^{(m\times1)}$

$x=\left[\begin{matrix}x^{(1)} \\ x^{(1)} \\ \vdots \\ x^{(m)}\end{matrix}\right]$


### Matrice X

$m$ contient l'ensemble du jeu de donn√©es avec $n$ caract√©ristiques (variables) pour les $m$ observations.

Fabrication √† partir des vecteurs $x$.

$X \in \mathbb{R}^{(mxn)}$

Variables multiples vecteur x.

$x^{(1)}=\left[\begin{matrix}x^{(1)}_n \\ \vdots\\ x^{(1)}_0\\ \end{matrix}\right]$

La matrice $X$ contient les vecteurs $x$ transpos√©s.

$X=\left[\begin{matrix}(x^{(1)})^{T} \\\ \vdots\\\ (x^{(m)})^{T} \\\ \end{matrix}\right]$

$X$ sous forme d√©velopp√©e

- Une ligne = observation
- Une colonne = caract√©ristique

$X=\left[\begin{matrix}x^{(1)}_n & \ldots & x^{(1)}_0 \\\ \vdots&\ddots&\vdots\\\ x^{(m)}_n & \ldots & x^{(m)}_0\\\ \end{matrix}\right]$

## Mod√®le
Le mod√®le est l'√©quation math√©matique qui permet de faire une pr√©vision.

Le mod√®le (ex ci-dessous) a des param√®tres ($a$,$b$) et une variable $x$.

$f(x)=ax+b$

Sous forme vectorielle (un seul vecteur)

$f=\Theta\cdot x$ ‚ö†Ô∏è

Sous forme matricielle (ensemble de vecteurs)

$f=X\Theta$ ‚ö†Ô∏è

## Th√©ta

Th√©ta est de vecteur de param√®tres du mod√®le.

$\Theta=\left[\begin{matrix}a\\\ b\end{matrix}\right]$

Dimension du vecteur
$(n+1)\times 1$


## Fonction co√ªt
Calcul de la moyenne des erreurs entre le mod√®le et le jeu de donn√©es ($y$).

L'erreur quadratique moyenne (MSE) mesure l‚Äô√©cart entre le r√©el et f(x) selon les param√®tre du mod√®le.

$$J(a,b) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m } (f ( x^{(i)} ) - y^{(i)})^2$$

- $i$ correspond √† une observation.
- $f ( x^{(i)})$ √† la valeur calcul√© par le mod√®le
- $y^{(i)}$ √† la valeur r√©elle du jeu de donn√©es.

soit

$J(a,b) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m }  ( ax^{(i)}+b-y^{(i)})^2$

Sous forme partiellement matricielle

$J(\Theta) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m }  ( X\cdot\Theta-y^{(i)})^2$


## Notions math√©matiques

### Fonction diff√©rentiables‚ö†Ô∏è
Une fonction diff√©rentiables est la g√©n√©ralisation polynomiale (dans un espace  vectoriel) de la notion de fonction d√©riv√©e √† une seule variable.

### D√©riv√©e directionnelle
La d√©riv√©e directionnelle quantifie une variation locale d'une fonction selon une direction d√©finie par un vecteur.

### D√©riv√©e partielle
La d√©riv√©e partielle est le cas particulier d'une d√©riv√©e directionnelle selon un des axes des coordonn√©es (pas exemple l'axe des absisses).

### Gradient ‚ö†Ô∏è
Le gradient est l'expression du maximum d'une d√©riv√©e directionnelle en un point donn√©.
Le gradient pointe vers la direction de la plus forte pente. Le gradient est un scalaire.


## D√©riv√©e partielle de la fonction co√ªt. üòÄÔ∏è

D√©riv√©e partielle de la fonction de co√ªt par rapport √† un param√©tre 
$\theta_j$

$$\frac{\partial{J(\Theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum_{i=1}^{m}x^{(i)}_j\times (\Theta^T x^{(i)}-y^{(i)})$$

Pour $a$

$\frac{\partial{J(a,b)}}{\partial{a}}=\frac{1}{m}\displaystyle\sum_{i=1}^{m}x^{(i)}\times (ax^{(i)}+b-y^{(i)})$      
Pour $b$

$\frac{\partial{J(a,b)}}{\partial{b}}=\frac{1}{m}\displaystyle\sum_{i=1}^{m} 1\times (ax^{(i)}+b-y^{(i)})$ 

Sous forme de matrice

$X=\left[\begin{matrix}x^{(1)}&1\\\ \vdots & \vdots\\\ x^{(m)}& 1 \end{matrix} \right]$  

$m \times ( n+1 )$

On prend la transpos√©e de $X$

$X^T=\left[ \begin{matrix} x^{(1)} & \ldots & x^{(m)}\\\ 1 & \ldots & 1\end{matrix} \right]$  

$(n+1) \times m$

### Vecteur gradient

$$\boxed{\nabla J(\Theta) = \frac{\partial{J(\Theta)}}{\partial{\Theta}}=X^T\cdot (X\cdot\Theta-y)}$$

$X^T$ la matrice $X$ transpos√©e (voir ci-dessus) correspond au $x$ et au 1 des √©quations (1) et (2) de dimension (n +1 x m).

$(X\cdot\Theta-y)$ est de dimension (m x 1).


## Descente de gradient ‚ö†Ô∏è üë∑‚Äç‚ôÇÔ∏èÔ∏è
On cherche √† minimiser les erreurs pour avoir un mod√®le fid√®le √† la r√©alit√©.
Voir [l\'algorithme de la descente de gradient](https://fr.wikipedia.org/wiki/Algorithme_du_gradient).

La **direction du gradient** $d$ est l'oppos√© du gradient $d=-\nabla f(x)$.


Mininum de la fonction ¬´ co√ªt ¬ª en apprentissage supervis√©.
- choix des param√®tres de fa√ßon al√©atoire (a,b).
- calcul de la pente = d√©riv√©e en $a_0$ $\frac{\partial{J(a_0)}}{\partial{a}}=0$
- avancement d'un pas $\alpha  = learning\ rate = vitesse\ de\ convergence\ (\ hyperparam√®tre\ )$


√Ä faire compl√©ter ci-dessous  ‚ö†Ô∏è üë∑‚Äç‚ôÇÔ∏èÔ∏è

$J(a,b)$ est une compos√©e de fonctions $(g \circ f)(a,b)$ (cela explique le principe de la d√©riv√©e) $f(a,b)=ax+b\ g(f)=f^2$

## Algorithme descente de gradient ‚ö†Ô∏è üë∑‚Äç‚ôÇÔ∏èÔ∏è
Algo de la descente de gradient

$\Theta=\Theta-\alpha\frac{\partial J(\Theta)}{\partial\Theta}$

$(n+1\times 1)$


## Biblioghaphie

- Aur√©lien G√©ron. (2019), *Deap Learning avec Keras et Tensorflow* (2e √©dition). DUNOD

## Sources

- [github : Aur√©lien G√©ron](https://github.com/ageron)
- [Machine learnia](https://machinelearnia.com/)
- [Produit matriciel](https://fr.wikipedia.org/wiki/Produit_matriciel)
- [Algorithme du gradient stochastique](https://fr.wikipedia.org/wiki/Algorithme_du_gradient_stochastique)
- Do andro√Øds dream of electric sheeps.
- Pluralitas non est ponenda sine necessitate.
