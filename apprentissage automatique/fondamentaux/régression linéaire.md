# R√©gression lin√©aire (en cours)
```
Les formules math√©matiques s'affichent dans Chrome avec l'extension ¬´ MathJax 3 Plugin for Github 
[https://chrome.google.com/webstore/detail/mathjax-3-plugin-for-gith/peoghobgdhejhcmgoppjpjcidngdfkod]¬ª.
```

## Introduction

M√©mo sur les fondamentaux de la r√©gression lin√©aire.

## Jeu de donn√©es (DATASET)

- m = nombre de lignes (observations),
- n = nombre de param√®tres (caract√©ristiques).

### Vecteurs y et x 

$y \in \mathbb{R}^{(m\times1)}$

$y=\left[\begin{matrix}y^{(1)} \\\ y^{(1)} \\\ \dots \\\ y^{(m)}\end{matrix}\right]$

$x \in \mathbb{R}^{(m\times1)}$

$x=\left[\begin{matrix}x^{(1)}\\\ x^{(1)}\\\ \dots\\\ x^{(m)}\end{matrix}\right]$


### Matrice X

$X$ contient l'ensemble du jeu de donn√©es avec $n$ caract√©ristiques (variables) pour les $m$ observations.

Fabrication √† partir des vecteurs x

$X \in \mathbb{R}^{(mxn)}$

Variables multiples (vecteur $x$).

$x^{(1)}=\left[\begin{matrix}x^{(1)}_n \\\ \vdots\\\ x^{(1)}_0\\\ \end{matrix}\right]$

La matrice $X$ contient les vecteurs $x$ transpos√©s.

$X=\left[\begin{matrix}(x^{(1)})^{T} \\\ \vdots\\\ (x^{(m)})^{T} \\\ \end{matrix}\right]$

$X$ sous forme d√©velopp√©e

- Une ligne = observation
- Une colonne = caract√©ristique

$X=\left[\begin{matrix}x^{(1)}_n & \dots & x^{(1)}_0\\\ \vdots&\ddots&\vdots\\\ x^{(m)}_n & \dots & x^{(m)}_0\\\ \end{matrix}\right]$

## Mod√®le
Le mod√®le est l'√©quation math√©matique qui permet de faire une pr√©vision.

Le mod√®le (ex ci-dessous) a des param√®tres ($a$,$b$) et une variable $x$.

$f(x)=ax+b$

Sous forme vectorielle

$f=X\cdot\Theta$

## Th√©ta

Th√©ta est de vecteur de param√®tres du mod√®le.

$\Theta=\left[\begin{matrix}a\\\ b\end{matrix}\right]$

Dimension du vecteur
$(n+1)\times 1$


## Fonction co√ªt
Calcul de la moyenne des erreurs entre le mod√®le et le jeu de donn√©es ($y$).

L'erreur quadratique moyenne (MSE) mesure l‚Äô√©cart entre le r√©el et f(x) selon les param√®tre du mod√®le.

$J(a,b) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m } (f ( x^{(i)} ) - y^{(i)})^2$

- $i$ correspond √† une observation.
- $f ( x^{(i)})$ √† la valeur calcul√© par le mod√®le
- $y^{(i)}$ √† la valeur r√©elle du jeu de donn√©es.

soit

$J(a,b) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m }  ( ax^{(i)}+b-y^{(i)})^2$

Sous forme partiellement matricielle

$J(\Theta) =\frac {1}{2 m} \displaystyle\sum_{ i = 1 }^{ m }  ( X\cdot\Theta-y^{(i)})^2$


## Notions math√©matiques

### Fonction diff√©rentiables
Une fonction diff√©rentiables est la g√©n√©ralisation polynomiale (dans un espace  vectoriel) de la notion de fonction d√©riv√©e pour une seule variable.

### D√©riv√©e directionnelle
La d√©riv√©e directionnelle quantifie la variation locale en fonction d'une direction d√©finie par un vecteur.

### D√©riv√©e partielle
La d√©riv√©e partielle est le cas particulier d'une d√©riv√©e directionnelle selon un des axes des coordonn√©es (pas exemple l'axe des absisses).

### Gradient
Le gradient est l'expression d'une d√©riv√©e directionnelle en un point donn√©.
Le gradient pointe vers la direction de la plus forte pente. Le gradient est un scalaire.


## D√©riv√©e partielle de la fonction co√ªt. üòÄÔ∏è

$\frac{\partial{J(a,b)}}{\partial{a}}=\frac{1}{m}\displaystyle\sum_{i=1}^{m}x^{(i)}\times (ax^{(i)}+b-y^{(i)})$      

$\frac{\partial{J(a,b)}}{\partial{b}}=\frac{1}{m}\displaystyle\sum_{i=1}^{m} 1\times (ax^{(i)}+b-y^{(i)})$ 

Sous forme de matrice

$X=\left[\begin{matrix}x^{(1)}&1\\\ \vdots & \vdots\\\ x^{(m)}& 1 \end{matrix} \right]$  

$m \times ( n+1 )$

On prend la transpos√©e de $X$

$X^T=\left[ \begin{matrix} x^{(1)} & \dots & x^{(m)}\\ 1 & \dots & 1\end{matrix} \right]$  

$(n+1) \times m$

$\frac{\partial{J(\Theta)}}{\partial{b}}=X^T\cdot (X\cdot\Theta-Y)$

$(n+1\times1)$

$X^T$ la matrice X transpos√©e (voir ci-dessus) correspond au x et au 1 des √©quations (1) et (2) de dimension (n +1 x m).

$(X\cdot\Theta-Y)$ est de dimension (m x 1).


## Descente de gradient ‚õîÔ∏è
Minimisation des erreurs pour avoir un mod√®le fid√®le √† la r√©alit√©.
Mininum de la fonction ¬´ co√ªt ¬ª en apprentissage supervis√©.
- choix des param√®tres de fa√ßon al√©atoire (a,b).
- calcul de la pente = d√©riv√©e en a0 $\frac{\partial{J(a_0)}}{\partial{a}}=0$
- avancement d'un pas $\alpha  = learning\ rate = vitesse\ de\ convergence\ (\ hyperparam√®tre\ )$


√Ä faire compl√©ter ci-dessous 

$J(a,b)$ est une compos√©e de fonctions $(g \circ f)(a,b)$ (cela explique le principe de la d√©riv√©e) $f(a,b)=ax+b\ g(f)=f^2$

## Algo matrice
Algo de la descente de gradient

$\Theta=\Theta-\alpha\frac{\partial J(\Theta)}{\partial\Theta}$

$(n+1\times 1)$


## Biblioghaphie

- Aur√©lien G√©ron. (2019), *Deap Learning avec Keras et Tensorflow* (2e √©dition). DUNOD

## Sources

- [github : Aur√©lien G√©ron](https://github.com/ageron)
- [Machine learnia](https://machinelearnia.com/)
- [Produit matriciel](https://fr.wikipedia.org/wiki/Produit_matriciel)
- [Algorithme du gradient stochastique](https://fr.wikipedia.org/wiki/Algorithme_du_gradient_stochastique)
